{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\leoni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "# from keras.preprocessing import text, sequence\n",
    "# from keras import layers, models, optimizers\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/tech_test_data-1.csv\").drop(\"message_id\", axis = 1)\n",
    "\n",
    "customer_df = df[df[\"message_source\"] == \"customer\"][[\"message\",\"case_type\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-processing Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Pre-Processing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(phraze):\n",
    "    new_phraze = []\n",
    "    for word, tag in pos_tag(word_tokenize(phraze)):\n",
    "        wntag = tag[0].lower()\n",
    "        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
    "        if not wntag:\n",
    "            new_phraze.append(word)\n",
    "        else:\n",
    "            new_phraze.append(lemmatizer.lemmatize(word, wntag))\n",
    "        \n",
    "    return \" \".join(new_phraze)\n",
    "\n",
    "def replace_num(phrase):\n",
    "    return re.sub(\" \\d+\", \" _number_\", phrase)\n",
    "\n",
    "def replace_orderID(phrase):\n",
    "    return re.sub(\"([A-Za-z]+[\\d@]+[\\w@]*|[\\d@]+[A-Za-z]+[\\w@]*)\", \"_orderID_\", phrase)\n",
    "\n",
    "def remove_punc(phraze):\n",
    "    return re.sub(r'[^\\w\\s]',\"\",phraze)\n",
    "\n",
    "def remove_extra_space(phraze):\n",
    "    return re.sub(' +', ' ', phraze)\n",
    "    \n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"hasn\\’t\", \"has not\", phrase)\n",
    "    phrase = re.sub(r\"haven\\’t\", \"has not\", phrase)\n",
    "    phrase = re.sub(r\"\\’d\", \" would\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 NLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLP_Preprocessor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, variables=None):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # we need the fit statement to accomodate the sklearn pipeline\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X['message'] = X['message'].apply(lambda x: x.lower()).\\\n",
    "                apply(lambda x: decontracted(x)).\\\n",
    "                apply(lambda x: \" \".join([item for item in x.split() if item not in stop_words])).\\\n",
    "                apply(lambda x: replace_num(x)).\\\n",
    "                apply(lambda x: replace_orderID(x)).\\\n",
    "                apply(lambda x: lemmatize(x)).\\\n",
    "                apply(lambda x: remove_punc(x)).\\\n",
    "                apply(lambda x: remove_extra_space(x))\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_processor = NLP_Preprocessor()\n",
    "cleaned_df = nlp_processor.fit_transform(customer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.4, random_state=seed)\n",
    "\n",
    "nlp_processor = NLP_Preprocessor()\n",
    "cleaned_df = nlp_processor.fit_transform(customer_df)\n",
    "\n",
    "X = cleaned_df.drop('case_type', axis=1)\n",
    "y = cleaned_df['case_type'].apply(lambda x: 0 if x==\"cancel_order\" else 1)\n",
    "\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X['message'], y)\n",
    "\n",
    "X_train = X_train[\"message\"]\n",
    "X_test = X_test[\"message\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
