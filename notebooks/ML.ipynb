{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\leoni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\leoni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "# from keras.preprocessing import text, sequence\n",
    "# from keras import layers, models, optimizers\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "# import pandas, xgboost, numpy, textblob, string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/tech_test_data-1.csv\").drop(\"message_id\", axis = 1)\n",
    "\n",
    "customer_df = df[df[\"message_source\"] == \"customer\"][[\"message\",\"case_type\"]]\n",
    "convos_df = df.groupby([ \"conversation_id\", \"case_type\"])[\"message\"].apply(lambda x: ' '.join(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey hey, I ordered something yesterday but it was the wrong item – can I still cancel that?'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_df[\"message\"].iloc[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'person'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(\"persons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP / Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(phraze):\n",
    "    new_phraze = []\n",
    "    for word, tag in pos_tag(word_tokenize(phraze)):\n",
    "        wntag = tag[0].lower()\n",
    "        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
    "        if not wntag:\n",
    "            new_phraze.append(word)\n",
    "        else:\n",
    "            new_phraze.append(lemmatizer.lemmatize(word, wntag))\n",
    "        \n",
    "    return \" \".join(new_phraze)\n",
    "\n",
    "def replace_num(phrase):\n",
    "    return re.sub(\" \\d+\", \" _number_\", phrase)\n",
    "\n",
    "def replace_orderID(phrase):\n",
    "    return re.sub(\"([A-Za-z]+[\\d@]+[\\w@]*|[\\d@]+[A-Za-z]+[\\w@]*)\", \"_orderID_\", phrase)\n",
    "\n",
    "def remove_punc(phraze):\n",
    "    return re.sub(r'[^\\w\\s]',\"\",phraze)\n",
    "\n",
    "def remove_extra_space(phraze):\n",
    "    return re.sub(' +', ' ', phraze)\n",
    "    \n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"hasn\\’t\", \"has not\", phrase)\n",
    "    phrase = re.sub(r\"haven\\’t\", \"has not\", phrase)\n",
    "    phrase = re.sub(r\"\\’d\", \" would\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "\n",
    "def preprocessing(df):\n",
    "    new_df = df.copy()\n",
    "    \n",
    "    \n",
    "    new_df['message'] = new_df['message'].apply(lambda x: x.lower()).\\\n",
    "        apply(lambda x: decontracted(x)).\\\n",
    "        apply(lambda x: \" \".join([item for item in x.split() if item not in stop_words])).\\\n",
    "        apply(lambda x: replace_num(x)).\\\n",
    "        apply(lambda x: replace_orderID(x)).\\\n",
    "        apply(lambda x: lemmatize(x)).\\\n",
    "        apply(lambda x: remove_punc(x)).\\\n",
    "        apply(lambda x: remove_extra_space(x))\n",
    "    \n",
    "    \n",
    "    return new_df\n",
    "\n",
    "\n",
    "clean_df = preprocessing(customer_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'worry order id _orderID_ let check account number'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df[\"message\"].iloc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Validation sets\n",
    "I am using Stratisfied split in order to make sure i have a balanced train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.4, random_state=seed)\n",
    "\n",
    "X = customer_df.drop('case_type', axis=1)\n",
    "y = customer_df['case_type'].apply(lambda x: 0 if x==\"cancel_order\" else 1)\n",
    "\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X['message'], y)\n",
    "\n",
    "X_train = X_train[\"message\"]\n",
    "X_test = X_test[\"message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6          yeah account number 09832453, order bsd932x0\n",
       "85                              account number 67223023\n",
       "49                              account number 01928340\n",
       "66    hi, i have an order due to arrive today and no...\n",
       "27    hello, i ordered from your service yesterday b...\n",
       "72    of course, order id 987yh512 and account numbe...\n",
       "8                      hello, i need to cancel an order\n",
       "41        order id nb0293rj and account number 09128342\n",
       "50                                   order id 09ba jh01\n",
       "31                 i’d like to cancel an order with you\n",
       "11                              account number 67223023\n",
       "15        order id 87ghe8eu and account number 98234321\n",
       "19                          yeah let me check two secs…\n",
       "64                                   order id 09ba jh01\n",
       "20                              account number 01928340\n",
       "52    hi, my order was supposed to arrive yesterday,...\n",
       "76        order id nb0293rj and account number 09128342\n",
       "21                                   order id 09ba jh01\n",
       "2     sure, my order id is a8b9v1e9 and account numb...\n",
       "78                  hey, do you know where my order is?\n",
       "63                              account number 01928340\n",
       "74    i’d like to find out where my order is guys, i...\n",
       "33           account number 85430982, order id nbreg923\n",
       "80              order id 09ba jh01 and account 09834532\n",
       "68    sure, my order id is a8b9v1e9 and account numb...\n",
       "84    no worries, my order id is bedsw912, let me ch...\n",
       "13    hey hey, i ordered something yesterday but it ...\n",
       "29        order id 987yh512 and account number 32430984\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "\n",
    "The next step is the feature engineering step. In this step, raw text data will be transformed into feature vectors and new features will be created using the existing dataset. We will implement the following different ideas in order to obtain relevant features from our dataset.\n",
    "\n",
    "2.1 Count Vectors as features <br />\n",
    "2.2 TF-IDF Vectors as features <br />\n",
    "2.2.1 Word level <br />\n",
    "2.2.2 N-Gram level <br />\n",
    "2.2.3 Character level <br />\n",
    "2.3 Word Embeddings as features <br />\n",
    "2.4 Text / NLP based features <br />\n",
    "2.5 Topic Models as features <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Count Vectors as features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(X['message'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "X_train_count =  count_vect.transform(X_train)\n",
    "X_test_count =  count_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 127)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2.2 TF-IDF Vectors as features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "tfidf_vect.fit(X['message'])\n",
    "\n",
    "X_train_tfidf =  tfidf_vect.transform(X_train)\n",
    "X_test_tfidf =  tfidf_vect.transform(X_test)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3))\n",
    "tfidf_vect_ngram.fit(X['message'])\n",
    "X_train_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "X_test_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3))\n",
    "tfidf_vect_ngram_chars.fit(X['message'])\n",
    "X_train_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train) \n",
    "X_test_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2.3 Word Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-190-3b626f7fe8ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# create a tokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'message'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mword_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('../data/wiki-news-300d-1M.vec',encoding=\"utf8\")):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(X['message'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "X_train_seq_x = sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=70)\n",
    "X_test_seq_x = sequence.pad_sequences(token.texts_to_sequences(X_test), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4  NLP based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['char_count'] = X['message'].apply(len)\n",
    "X['word_count'] = X['message'].apply(lambda x: len(x.split()))\n",
    "X['word_density'] = X['char_count'] / (X['word_count']+1)\n",
    "X['punctuation_count'] = X['message'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "X['title_word_count'] = X['message'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "X['upper_case_word_count'] = X['message'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_family = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "# function to check and get the part of speech tag count of a words in a given sentence\n",
    "def check_pos_tag(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_family[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt\n",
    "\n",
    "X['noun_count'] = X['message'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "X['verb_count'] = X['message'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "X['adj_count'] = X['message'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "X['adv_count'] = X['message'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "X['pron_count'] = X['message'].apply(lambda x: check_pos_tag(x, 'pron'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Topic Models as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a LDA Model\n",
    "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "X_topics = lda_model.fit_transform(X_train_count)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vect.get_feature_names()\n",
    "\n",
    "# view the topic models\n",
    "n_top_words = 5\n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0,1]\n",
    "s1, d2 = a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(clf,clf_name, is_neural_net=False):\n",
    "    \n",
    "    data = {\"count\": [X_train_count,X_test_count ],\n",
    "            \"tfidf\": [X_train_tfidf, X_test_tfidf],\n",
    "            \"ngram\": [X_train_tfidf_ngram, X_test_tfidf_ngram],\n",
    "            \"chars\": [X_train_tfidf_ngram_chars, X_test_tfidf_ngram_chars]\n",
    "           }\n",
    "    print(clf_name)\n",
    "    print(\"-------\")\n",
    "    for k in data.keys():\n",
    "        train, test = data[k]\n",
    "        # fit the training dataset on the classifier\n",
    "        clf.fit(train, y_train)\n",
    "\n",
    "        # predict the labels on validation dataset\n",
    "        y_pred = clf.predict(test)\n",
    "\n",
    "        if is_neural_net:\n",
    "            y_pred = y_pred.argmax(axis=-1)\n",
    "\n",
    "        acc = metrics.accuracy_score(y_pred, y_test)\n",
    "        print(\"{} Accuracy: {}\".format(k, acc))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "classsifiers = {\"Naive Bayes\":naive_bayes.MultinomialNB(),\n",
    "                \"Logistic Regression\":linear_model.LogisticRegression(random_state=seed),\n",
    "                \"SVM\": svm.SVC(random_state=seed),\n",
    "                \"rfc\": ensemble.RandomForestClassifier(random_state=seed),\n",
    "                \"gbc\": ensemble.GradientBoostingClassifier(random_state=seed)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "-------\n",
      "count Accuracy: 0.42105263157894735\n",
      "tfidf Accuracy: 0.47368421052631576\n",
      "ngram Accuracy: 0.42105263157894735\n",
      "chars Accuracy: 0.5263157894736842\n",
      "\n",
      "Logistic Regression\n",
      "-------\n",
      "count Accuracy: 0.5263157894736842\n",
      "tfidf Accuracy: 0.47368421052631576\n",
      "ngram Accuracy: 0.5263157894736842\n",
      "chars Accuracy: 0.5789473684210527\n",
      "\n",
      "SVM\n",
      "-------\n",
      "count Accuracy: 0.631578947368421\n",
      "tfidf Accuracy: 0.5789473684210527\n",
      "ngram Accuracy: 0.42105263157894735\n",
      "chars Accuracy: 0.5263157894736842\n",
      "\n",
      "rfc\n",
      "-------\n",
      "count Accuracy: 0.631578947368421\n",
      "tfidf Accuracy: 0.5263157894736842\n",
      "ngram Accuracy: 0.47368421052631576\n",
      "chars Accuracy: 0.5789473684210527\n",
      "\n",
      "gbc\n",
      "-------\n",
      "count Accuracy: 0.631578947368421\n",
      "tfidf Accuracy: 0.631578947368421\n",
      "ngram Accuracy: 0.5789473684210527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\digitalgenius\\venv\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "d:\\projects\\digitalgenius\\venv\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "d:\\projects\\digitalgenius\\venv\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "d:\\projects\\digitalgenius\\venv\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "d:\\projects\\digitalgenius\\venv\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "d:\\projects\\digitalgenius\\venv\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars Accuracy: 0.3684210526315789\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for clf_name in classsifiers.keys():\n",
    "    train_model(classsifiers[clf_name], clf_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\leoni\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 3s 86ms/step - loss: 0.6907\n",
      "RNN-LSTM, Word Embeddings 0.4666666666666667\n"
     ]
    }
   ],
   "source": [
    "def create_rnn_lstm():\n",
    "\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "clf = create_rnn_lstm()\n",
    "accuracy = train_model(clf, X_train_seq_x, y_train, X_test_seq_x, is_neural_net=True)\n",
    "print(\"RNN-LSTM, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
